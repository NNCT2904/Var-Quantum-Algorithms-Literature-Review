\section{Contributions and Future Works}

We have implemented three methods of dealing with barren plateau by altering the ansatz's depth, cost function, and initial parameters aspects.
The experiments have produced the results as the slopes of the gradients for a number of qubits, as well as the performance of ansatzes in neural network training.
The results indicate that the variances of the gradient can be stable if we set a limit on the length of the circuit and the cost function, and the training performance can increase if we carefully select the initial parameters.

With this artefact, we have addressed the research design in Section \ref{Research Design section}.
We have implemented the three methods, namely \textit{local cost function, shallow ansatz depth}, \textit{layerwise learning} and \textit{identity blocks}.
We have compared the variances in the first phase of the experiment.
According to Figure \ref{Fig: Plot Variances}, the variance values of the limited depth-cost function method at seven qubits configuration is significantly higher than the rest.
We anticipate that a higher variance value does not mean the optimisation is going in the right direction, the model can still stick it in a local minimum, or the error landscape is random.
To verify the performance of these methods, in the second phase, we implemented a variational quantum neural network to solve a classification problem with a standard dataset.

Phase two of the experiments has verified that the unrestricted configuration may have run into a barren plateau, while the others can converge to their minimums - the answers.
Moreover, these experiments are conducted in an emulated environment with a noise model from \emph{ibm\_perth} backend.
Thus, this experiment can reflect the real-life situation to some extent.


So far we have tested the ansatzes with an extended number of layers.
However, the number of qubits is also a factor that leads to barren plateaus.
While it is possible to run the experiment with higher qubit counts to replicate the result by Cereze et al. \cite{cerezoCostFunctionDependent2021}, due to constraints in time and computation resources, we only use up to five qubits with COBYLA optimiser.
The research also opens a new question about the relationship between model capacity, datasets and barren plateau.
Furthermore, we have not covered the performance comparison between identity blocks and the real amplitude circuit in general use cases.
These issues are left for further studies and experiments.

% According to Figure \ref{Fig: Plot Variances}, the variance values of the limited depth - cost function method at seven qubits configuration is significantly higher than the rest.
% At higher qubit configuration, the barren plateaus are also more likely to appear, and the performance of these methods under this phenomenon is worth investigating.
