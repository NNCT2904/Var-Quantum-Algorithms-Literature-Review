\section{Comparison and Disscussion}
\begin{itemize}
    \item make a table;
    \item Characteristics, pros and cons;
    \item Highlight which occasion for which method, context of QNN
    \item Comparison, compact way
\end{itemize}

We now discuss the characteristics, pros and cons of each methods.
Let's recall the main cause of the Barren Plateaus phenomenon in VQA and QNN development are the ansatz depth and the randomised starting parameter.
For this reason, we will identify the characteristics of each method based on how they address these two factors.

The paper \cite{cerezoCostFunctionDependent2021} by Cerezo et al. address the issue by propose their own Alternating Layered Ansatz with a defined upper-bound and lower-bound for circuit depth to work with their local cost function. 
The initialise parameters can be randomly configured.
Thus, this methods can be use to train a shallow circuit of length in the defined bound. 
Overall, this is an ansatz design that does not exhibit a Barren Plateaus, at the cost of circuit depth.

Grant et al. \cite{grantInitializationStrategyAddressing2019} choose to mitigate the issue by addressing the initialisation parameters. 
In short, the authors' idea was to divide the ansatz into shallow blocks, and configure the parameters such that each block is implemented as an identity operation.
The authors' aim was to avoid non-zero gradient in the first training iteration, which guarantee the initialization away from a Barren Plateaus and allow the circuit to be trained efficiently.
However this method cannot ensure the algorithm will not goes into a Barren Plateau again, because the Barren Plateaus phenomenon is cost function dependent \cite{cerezoCostFunctionDependent2021}.
Overall, the authors manipulated the starting parameters to achieve trainability, but the risk of going to a plateau is still there.

Interestingly, the method by Skolik et al. \cite{skolikLayerwiseLearningQuantum2021} seems to address the both factors.
The first phase of the algorithm creates an ansatz by adding many shallow layers of zero parameterized circuit. 
Then the second phase train the circuit for a portion at a time.
As the randomness is contained to shallower sub-circuits, the chance of going to a plateaus is minimized during training.
We can see that this scheme is more advantageous when training the full circuit is not an viable option.