\section{Comparison and Discussion}
We will now discuss the main characteristics, the pros and cons of each of the previously discussed methods of dealing with barren plateaus while training VQAs and QNNs in particular.
As has been established above, the main causes of the barren plateaus phenomenon include: the excessive circuit depth, the large number of qubits in use and poorly initialised circuit parameters.
Consequently, we will identify characteristics of each method based on how they address these factors.
Table \ref{quick comparison of methods} provides the methods comparison in a concise form.

There are two distinct approaches to dealing with the ansatz depth, either setting the bounds \cite{cerezoCostFunctionDependent2021} or allowing unlimited depth \cite{skolikLayerwiseLearningQuantum2021, grantInitializationStrategyAddressing2019}.
The first approach is studied by Cerezo et al. \cite{cerezoCostFunctionDependent2021}.
The authors discovered that the gradient vanishes at different rate depends on the circuit depth, so they define the bounds on the circuit depth such that the cost function landscape can maintain its slope.
We have two implementations of the second approach, they both have the unlimited ansatz depth but different in the construction procedure.
Skolik et al. \cite{skolikLayerwiseLearningQuantum2021} construct the ansatz by adding consecutive layers until there is no improvement in training efficiency.
On the other hand, Grant et al. \cite{grantInitializationStrategyAddressing2019} train the circuit without altering any properties of the ansatz.

There are two main techniques developed to deal with the cost function.
The first \emph{global cost function} compares the states of all qubits used in the ansatz, this cost function is widely used in QNN development.
Skolik et al. and Grant et al. also adopted this cost function in their studies.
The second technique \emph{local cost function} compares the states of some qubits used in the ansatz, this cost function is studied by Cerezo et al. \cite{cerezoCostFunctionDependent2021} in combination with the depth-bounded ansatz.

The initial parameters can be either randomised, or predefined.
As the gradient is calculated with those parameters, randomised parameters can land into a barren plateaus.
However this is not the case in the study of Cerezo et al. which completely eliminate the near-zero gradient phenomenon.
Since the plateau does not exist and the gradient can maintain its slope, the randomised initial parameter is a viable option.
For the case of predefined initial parameters, we have two studies.
Skolik et al. \cite{skolikLayerwiseLearningQuantum2021} proposed an algorithm that starts with zeros parameters in a small ansatz.
Their algorithm allows the parameters to be trained as the ansatz grow over time.
Grant et al. \cite{grantInitializationStrategyAddressing2019} divide the ansatz into shallow blocks and configure the parameters such that each block implement an identity operation.
The authors aimed to avoid non-zero gradients in the first training iteration, which guaranteed the initialisation away from a barren plateau and allowed the circuit to be trained efficiently.
However, this method cannot ensure the algorithm will not go into a barren plateau again in further iterations of optimization algorithm.
Overall, the authors manipulated the starting parameters to achieve trainability, but the risk of going to a plateau is still there.

Finally, it is worth mentioning that Quantum Convolutional Neural Networks (QCNNs) and QNNs with tensor network architectures do not have Barren Plateaus \cite{congQuantumConvolutionalNeural2019}.

\begin{table*}
    \centering
    \begin{tabular}{||p{3cm} p{2cm} p{2cm} p{2cm} p{2cm}||}
        \hline
        \textbf{Method /\newline Aspect}             & \textbf{Local cost function, shallow circuits} & \textbf{Identity Blocks} & \textbf{Layerwise learning} & \textbf{QCNN} \\
        \hline \hline
        \raggedright\emph{Ansatz depth}              & Bounded                                        & Any length               & Any length                  & Any length    \\
        \raggedright\emph{Qubits to be measured}     & Limited                                        & All qubits               & All qubits                  & All qubits    \\
        \raggedright\emph{Initial Parameters}        & Randomised                                     & Restricted               & Restricted                  & Randomised    \\
        \hline \hline
        \raggedright\emph{Effect to Barren Plateaus} & Eliminated                                     & Avoided                  & Avoided                     & Eliminated    \\
        \hline
    \end{tabular}
    \caption{A concise comparison of the reviewed methods.}
    \label{quick comparison of methods}
\end{table*}
