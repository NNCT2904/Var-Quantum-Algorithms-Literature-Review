\section{Comparison and Discussion}
We now discuss the characteristics, pros and cons of each method.
Let's recall the main cause of the Barren Plateaus phenomenon in VQA and QNN development are the ansatz depth, the number of qubit and the randomised starting parameter.
For this reason, we will identify the characteristics of each method based on how they address these two factors.
The Table \ref{quick comparison of methods} provides the comparison in a compact form.

Cerezo et al. \cite{cerezoCostFunctionDependent2021} addressed the issue by proposing their own Alternating Layered Ansatz with a defined upper-bound and lower-bound for circuit depth.
The ansatz, in combination with a local cost function which only measure a restricted number of qubit, has proven that the Barren Plateaus can be fully eliminated. 
The initialised parameters can be randomly configured.
Thus, this method can be used to train a shallow circuit of length in the defined bound. 

Grant et al. \cite{grantInitializationStrategyAddressing2019} choose to mitigate the issue by addressing the initialisation parameters. 
In short, the authors' idea was to divide the ansatz into shallow blocks and configure the parameters such that each block is implemented as an identity operation.
The authors aimed to avoid non-zero gradients in the first training iteration, which guaranteed the initialisation away from a Barren Plateaus and allowed the circuit to be trained efficiently.
However, this method cannot ensure the algorithm will not go into a Barren Plateau again because the Barren Plateaus phenomenon is cost function dependent \cite{cerezoCostFunctionDependent2021}.
Overall, the authors manipulated the starting parameters to achieve trainability, but the risk of going to a plateau is still there.

The method by Skolik et al. \cite{skolikLayerwiseLearningQuantum2021} seems to address both factors.
The first phase of the algorithm creates an ansatz by adding many shallow layers of zero parameterised circuits. 
Then the second phase trains the circuit for a portion at a time.
As the randomness is contained to shallower sub-circuits, the chance of going to a plateau is minimised during training.
We can see that this scheme is more advantageous when training the whole circuit is not a viable option. 
Once again, the Barren Plateaus is mitigated, not eliminated, compared to the method in \cite{cerezoCostFunctionDependent2021}.

Finally, it is worth mentioning that Quantum Convolutional Neural Networks (QCNNs) and QNNs with tensor network architectures do not have Barren Plateaus, as pointed out in \cite{congQuantumConvolutionalNeural2019}.

\begin{table}[]
    \centering
    \begin{tabular}{|p{2cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
        \hline
        Method              & Local cost function, shallow circuits & Identity Blocks   & Layerwise learning    & QCNN \\
        \hline
        Ansatz depth        & Bounded                               & Any               & Any                   & Any \\
        \hline
        Qubits to be measured    & Limited                          & All qubits        & All qubits            & All qubits \\
        \hline
        Initial Parameters  & Randomised                            & Restricted        & Restricted            & Randomised \\
        \hline
        Barren Plateaus     & Eliminated                            & Avoided           & Avoided               & Eliminated \\
        \hline
    \end{tabular}
    \caption{A compact comparison of the reviewed methods}
    \label{quick comparison of methods}
\end{table}