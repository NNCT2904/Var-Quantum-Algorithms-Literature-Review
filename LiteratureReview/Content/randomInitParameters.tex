\subsection{Addressing Random Initialisation Parameters}

\subsubsection{Layerwise learning}
The algorithm in \cite{skolikLayerwiseLearningQuantum2021} starts with training a shallow circuit to its best then extends the circuit and uses the best parameter achieved in the previous step for the newly extended circuit. 
Note that only the new parameters are allowed to vary while the previous parameters are frozen.
Thus, the randomness is contained to only shallow sub-circuits in phase one during the whole routine and minimises the probability of meeting a plateau.


\subsubsection{Identity blocks}

Grant et al. \cite{grantInitializationStrategyAddressing2019} have addressed the random initialization parameter so that the early training steps do not start on a plateau region. 
The idea is to divide the big parameterized circuit into blocks and initialise each block so that the result is a fixed unitary matrix, i.e., an identity operation. 
The method is as follows: partition the circuit into $M$ blocks of depth $L$; for each block, initiate some parameters with random values, and fix the remaining parameters in a way that results in a fixed unitary matrix, i.e., invert the first part so that each block implements an identity operation.

For any $m = 1, \cdots, M$, the parameters $\theta$, the block is presented as:
\begin{equation}
    U_m(\theta_m)
    = \prod_{l=L}^1 U_l(\theta_{l,1}^m) \prod_{l=1}^L U_l(\theta_{l,2}^m).
\end{equation}
The starting parameters $\theta_{l,1}$ can be chosen randomly, while the values for $\theta_{l,2}$ are chosen such that $U_l(\theta_{l,2}) = U_l(\theta_{l,1})^\dagger$:
\begin{equation}
    U_m(\theta_m)
    = \prod_{l=L}^1 U_l(\theta_{l,1}^m)
    \prod_{l=1}^L U_l(\theta_{l,1}^m)^\dagger
    = I_m,
\end{equation}
resulting identity blocks for the circuit (see Figure \ref{identity block}):
\begin{equation}
    U(\theta^{init})
    = \prod_{m=M}^1 U_m(\theta_m)
    = \prod_{m=M}^1 I_m
    = I.
\end{equation}

This initialization strategy does not guarantee that the optimization algorithm will not go into a BP afterwards. 
It, however, ensures avoiding generation of zero gradients for the majority of parameters and allows the parameterized circuit to be trained more efficiently.

\begin{figure} 
    \centerline{
        \Qcircuit @C=1em @R=0em {
        & \multigate{4}{U_l(\theta_{l,1}^m)}    & \multigate{4}{U_l(\theta_{l,2}^m)}    & \qw \\
        & \ghost{U_l(\theta_{l,1}^m)}           & \ghost{U_l(\theta_{l,2}^m)}           & \qw \\
        & \ghost{U_l(\theta_{l,1}^m)}           & \ghost{U_l(\theta_{l,2}^m)}           & \qw \\
        & \ghost{U_l(\theta_{l,1}^m)}           & \ghost{U_l(\theta_{l,2}^m)}           & \qw \\
        & \ghost{U_l(\theta_{l,1}^m)}           & \ghost{U_l(\theta_{l,2}^m)}           & \qw \\
        {\gategroup{1}{2}{5}{3}{.4em}{--}}\\
        }
    }
    \centerline{Identity block}
    \caption{
        An example of an Identity block.
        The initial parameters $\theta_{l,1}$ for the $U_l(\theta_{l,1})$ can be chosen at random, 
        while the values $\theta_{l,2}$ for $U_l(U_l(\theta_{l,1}))$ are fixed such that  $U_l(\theta_{l,1}) U_l(\theta_{l,1}) = I$.
    }\label{identity block}
\end{figure}